%%% defines and solves cost minimization/reward maximization LP

%%% Inputs:
% TF: transition function (usually P matrix generated by grid_world.m)
% init: Initial state distribution
% absorb: absorbing states (should be sorted in increasing order !)
%         (should be given in the form of state number not coordinates !)
% target: target states (should be given in the form of state number not coordinates !)
% cost: cost function (should have size=(state*action,1) )

function [policy,lambda,cvx_optval]=min_cost_LP(TF,init,absorb,target,cost)
    num_states=size(TF,1);
    act=size(TF,3);
    A=zeros(num_states,num_states*act);
    Tar_states=zeros(length(target),num_states*act);
    I=zeros(num_states,num_states*act);
    for k=1:act
        A(:,num_states*(k-1)+1:num_states*k)=TF(:,:,k)';
        I(:,num_states*(k-1)+1:num_states*k)=eye(num_states);
    end
    for s=size(target,1)
        Tar_states(target(s,1),:)=A(target(s,1),:);
    end
    for s=size(absorb,1)
        A(absorb(s,1),:)=[];
        I(absorb(s,1),:)=[];
        init(absorb(s,1))=[];
    end
    
    
    cvx_solver MOSEK
    cvx_begin
        variables lambda(num_states*act) 
        disp(size(lambda))
        maximize( sum(cost'*lambda) );
        lambda>=0;
        (I-A)*lambda==init;

    cvx_end
    
    exp_times=zeros(num_states,1);
    policy=zeros(num_states,act);
    for s=1:num_states
        for a=1:act
          exp_times(s,1)=exp_times(s,1)+lambda(((a-1)*num_states)+s);
          if s==7
          end
        end
        if exp_times(s,1)~=0
             for a=1:act
                policy(s,a)=lambda((a-1)*num_states+s)/exp_times(s,1);
             end
        else
            policy(s,1)=1;
        end
        
    end
    
end